{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v5ChmLicLhzI"
   },
   "source": [
    "# Synopsis\n",
    "\n",
    "Here we create a TFIDF matrix from our corpus of novels, and then save a reduced version of this for use in HCA and PCA models. We choose to limit our vocabulary to 1000 top words, based on high-frequency non-stopwords.\n",
    "\n",
    "We begin by extracting a bag-of-words from the token table. Note that we could have chosen a different set of \"bags,\" e.g. paragraphs or an arbitrary chunk of n tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yWADUMoUp8_p"
   },
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PcwLF3YyqmVZ"
   },
   "outputs": [],
   "source": [
    "db_name = '../../data/novels.db'\n",
    "\n",
    "OHCO = ['genre', 'author', 'book', 'chapter', 'para_num', 'sent_num', 'token_num']\n",
    "GENRS = OHCO[:1]\n",
    "AUTHS = OHCO[:2]\n",
    "BOOKS = OHCO[:3]\n",
    "CHAPS = OHCO[:4]\n",
    "PARAS = OHCO[:5]\n",
    "SENTS = OHCO[:6]\n",
    "\n",
    "BAG = CHAPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LLdKbNbMLmI1"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YbadDvzWW0Sw"
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YbadDvzWW0Sw"
   },
   "source": [
    "# Pragmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YbadDvzWW0Sw"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EYW-zAG1LpTP",
    "toc-hr-collapsed": false
   },
   "source": [
    "# Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hefj5mkCLo0D"
   },
   "outputs": [],
   "source": [
    "with sqlite3.connect(db_name) as db:\n",
    "    tokens = pd.read_sql('SELECT * FROM token', db, index_col=OHCO)\n",
    "    vocab = pd.read_sql('SELECT * FROM vocab', db, index_col='term_id')\n",
    "    docs = pd.read_sql('SELECT * FROM doc', db, index_col=CHAPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3WoDnI5rmg6h",
    "toc-hr-collapsed": false
   },
   "source": [
    "## Create DTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3WoDnI5rmg6h"
   },
   "source": [
    "### Create word mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OJQvJsdwTIra"
   },
   "outputs": [],
   "source": [
    "WORDS = (tokens.punc == 0) & (tokens.num == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F9-E_3-WmjzR"
   },
   "source": [
    "### Extrct BOW from tokens\n",
    "\n",
    "To extract a bag-of-words model from our tokens table, we apply a simple `groupby()` operation. Note that we can drop in our hyperparameters easily -- CHAPS and 'term_id' and be replaced. We can easily write a function to simplify this process and make it more configurable. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ax8ePBkk4qP"
   },
   "outputs": [],
   "source": [
    "BOW = tokens[WORDS].groupby(BAG + ['term_id'])['term_id'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tiOTaBLCmmKP"
   },
   "source": [
    "### Convert BOW to DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I4tYnLT5lu0J"
   },
   "outputs": [],
   "source": [
    "DTM = BOW.unstack().fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "foZvi_brzyNR"
   },
   "source": [
    "### Create Bags table\n",
    "\n",
    "The bags table stores the OHCO content for each doc, since we remove this from the DTM. We can add some stats to this table if we wanted to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1TgoLS_xIu_a"
   },
   "outputs": [],
   "source": [
    "bags = pd.DataFrame(index = DTM.index)\n",
    "# bags['term_count'] = DTM.sum(1)\n",
    "# bags['tf'] = bags.term_count / bags.term_count.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1TgoLS_xIu_a"
   },
   "outputs": [],
   "source": [
    "DTM = DTM.reset_index(drop=True)\n",
    "DTM.index.name = 'bag_id'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TbYmZJUO_oKx",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Compute Term Frequencies and Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7IBFLaQODVPH"
   },
   "source": [
    "### Compute TF\n",
    "\n",
    "Note that TF is just the term count. It is often normalized in the computing the value, but it is defined as the count in the context of information retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-3vd2eNs-fif"
   },
   "source": [
    "### Compute IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-3vd2eNs-fif"
   },
   "outputs": [],
   "source": [
    "N_docs = DTM.shape[0]\n",
    "vocab['df'] = DTM[DTM > 0].count()\n",
    "vocab['idf'] = np.log10(N_docs / vocab.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-3vd2eNs-fif"
   },
   "source": [
    "### Test: View most frequent non-stops by IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-3vd2eNs-fif"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-345dbcc008cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'idf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "vocab[vocab.stop==0].sort_values('n', ascending=False).head(500)\\\n",
    "    .sort_values('idf', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6pFEbf5NAbvl"
   },
   "source": [
    "### Compute TFIDF\n",
    "\n",
    "See [Simone Teufel's lectures](https://www.cl.cam.ac.uk/teaching/1415/InfoRtrv/lecture4.pdf)\n",
    "\n",
    "```\n",
    "TF: term count\n",
    "N: number of docs\n",
    "DF: number of docs with term\n",
    "log = log10\n",
    "\n",
    "(1 + log(TF)) * log( N / DF)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-3vd2eNs-fif"
   },
   "outputs": [],
   "source": [
    "TFIDF = DTM * vocab['idf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-3vd2eNs-fif"
   },
   "source": [
    "### Test: Stopwords Detected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-3vd2eNs-fif"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term_str</th>\n",
       "      <th>n</th>\n",
       "      <th>p</th>\n",
       "      <th>port_stem</th>\n",
       "      <th>stop</th>\n",
       "      <th>df</th>\n",
       "      <th>idf</th>\n",
       "      <th>tfidf_sum</th>\n",
       "      <th>tfidf_mean</th>\n",
       "      <th>tfidf_max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>28533</td>\n",
       "      <td>0.019017</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>and</td>\n",
       "      <td>44991</td>\n",
       "      <td>0.029986</td>\n",
       "      <td>and</td>\n",
       "      <td>1</td>\n",
       "      <td>320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1328</th>\n",
       "      <td>as</td>\n",
       "      <td>11252</td>\n",
       "      <td>0.007499</td>\n",
       "      <td>as</td>\n",
       "      <td>1</td>\n",
       "      <td>320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>be</td>\n",
       "      <td>8787</td>\n",
       "      <td>0.005856</td>\n",
       "      <td>be</td>\n",
       "      <td>1</td>\n",
       "      <td>320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3209</th>\n",
       "      <td>but</td>\n",
       "      <td>9528</td>\n",
       "      <td>0.006350</td>\n",
       "      <td>but</td>\n",
       "      <td>1</td>\n",
       "      <td>320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3237</th>\n",
       "      <td>by</td>\n",
       "      <td>6923</td>\n",
       "      <td>0.004614</td>\n",
       "      <td>by</td>\n",
       "      <td>1</td>\n",
       "      <td>320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9681</th>\n",
       "      <td>for</td>\n",
       "      <td>11150</td>\n",
       "      <td>0.007431</td>\n",
       "      <td>for</td>\n",
       "      <td>1</td>\n",
       "      <td>320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10024</th>\n",
       "      <td>from</td>\n",
       "      <td>6780</td>\n",
       "      <td>0.004519</td>\n",
       "      <td>from</td>\n",
       "      <td>1</td>\n",
       "      <td>320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11006</th>\n",
       "      <td>had</td>\n",
       "      <td>12858</td>\n",
       "      <td>0.008570</td>\n",
       "      <td>had</td>\n",
       "      <td>1</td>\n",
       "      <td>320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11241</th>\n",
       "      <td>have</td>\n",
       "      <td>9099</td>\n",
       "      <td>0.006064</td>\n",
       "      <td>have</td>\n",
       "      <td>1</td>\n",
       "      <td>320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12267</th>\n",
       "      <td>in</td>\n",
       "      <td>23501</td>\n",
       "      <td>0.015663</td>\n",
       "      <td>in</td>\n",
       "      <td>1</td>\n",
       "      <td>320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13157</th>\n",
       "      <td>it</td>\n",
       "      <td>18445</td>\n",
       "      <td>0.012293</td>\n",
       "      <td>it</td>\n",
       "      <td>1</td>\n",
       "      <td>320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16135</th>\n",
       "      <td>no</td>\n",
       "      <td>5200</td>\n",
       "      <td>0.003466</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16213</th>\n",
       "      <td>not</td>\n",
       "      <td>10069</td>\n",
       "      <td>0.006711</td>\n",
       "      <td>not</td>\n",
       "      <td>1</td>\n",
       "      <td>320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16459</th>\n",
       "      <td>of</td>\n",
       "      <td>42638</td>\n",
       "      <td>0.028417</td>\n",
       "      <td>of</td>\n",
       "      <td>1</td>\n",
       "      <td>320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16539</th>\n",
       "      <td>on</td>\n",
       "      <td>8689</td>\n",
       "      <td>0.005791</td>\n",
       "      <td>on</td>\n",
       "      <td>1</td>\n",
       "      <td>320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24121</th>\n",
       "      <td>that</td>\n",
       "      <td>21120</td>\n",
       "      <td>0.014076</td>\n",
       "      <td>that</td>\n",
       "      <td>1</td>\n",
       "      <td>320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24127</th>\n",
       "      <td>the</td>\n",
       "      <td>85329</td>\n",
       "      <td>0.056870</td>\n",
       "      <td>the</td>\n",
       "      <td>1</td>\n",
       "      <td>320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24229</th>\n",
       "      <td>this</td>\n",
       "      <td>7512</td>\n",
       "      <td>0.005007</td>\n",
       "      <td>thi</td>\n",
       "      <td>1</td>\n",
       "      <td>320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24470</th>\n",
       "      <td>to</td>\n",
       "      <td>45176</td>\n",
       "      <td>0.030109</td>\n",
       "      <td>to</td>\n",
       "      <td>1</td>\n",
       "      <td>320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        term_str      n         p port_stem  stop   df  idf  tfidf_sum  \\\n",
       "term_id                                                                  \n",
       "0              a  28533  0.019017         a     1  320  0.0        0.0   \n",
       "862          and  44991  0.029986       and     1  320  0.0        0.0   \n",
       "1328          as  11252  0.007499        as     1  320  0.0        0.0   \n",
       "1987          be   8787  0.005856        be     1  320  0.0        0.0   \n",
       "3209         but   9528  0.006350       but     1  320  0.0        0.0   \n",
       "3237          by   6923  0.004614        by     1  320  0.0        0.0   \n",
       "9681         for  11150  0.007431       for     1  320  0.0        0.0   \n",
       "10024       from   6780  0.004519      from     1  320  0.0        0.0   \n",
       "11006        had  12858  0.008570       had     1  320  0.0        0.0   \n",
       "11241       have   9099  0.006064      have     1  320  0.0        0.0   \n",
       "12267         in  23501  0.015663        in     1  320  0.0        0.0   \n",
       "13157         it  18445  0.012293        it     1  320  0.0        0.0   \n",
       "16135         no   5200  0.003466        no     1  320  0.0        0.0   \n",
       "16213        not  10069  0.006711       not     1  320  0.0        0.0   \n",
       "16459         of  42638  0.028417        of     1  320  0.0        0.0   \n",
       "16539         on   8689  0.005791        on     1  320  0.0        0.0   \n",
       "24121       that  21120  0.014076      that     1  320  0.0        0.0   \n",
       "24127        the  85329  0.056870       the     1  320  0.0        0.0   \n",
       "24229       this   7512  0.005007       thi     1  320  0.0        0.0   \n",
       "24470         to  45176  0.030109        to     1  320  0.0        0.0   \n",
       "\n",
       "         tfidf_mean  tfidf_max  \n",
       "term_id                         \n",
       "0               0.0        0.0  \n",
       "862             0.0        0.0  \n",
       "1328            0.0        0.0  \n",
       "1987            0.0        0.0  \n",
       "3209            0.0        0.0  \n",
       "3237            0.0        0.0  \n",
       "9681            0.0        0.0  \n",
       "10024           0.0        0.0  \n",
       "11006           0.0        0.0  \n",
       "11241           0.0        0.0  \n",
       "12267           0.0        0.0  \n",
       "13157           0.0        0.0  \n",
       "16135           0.0        0.0  \n",
       "16213           0.0        0.0  \n",
       "16459           0.0        0.0  \n",
       "16539           0.0        0.0  \n",
       "24121           0.0        0.0  \n",
       "24127           0.0        0.0  \n",
       "24229           0.0        0.0  \n",
       "24470           0.0        0.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[TFIDF.sum() == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Iw1Rbl7FNMwX"
   },
   "source": [
    "### Add stats to Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a6EXs82eK4Oj"
   },
   "outputs": [],
   "source": [
    "vocab['tfidf_sum'] = TFIDF.sum()\n",
    "vocab['tfidf_mean'] = TFIDF.mean()\n",
    "vocab['tfidf_max'] = TFIDF.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a6EXs82eK4Oj",
    "toc-hr-collapsed": false
   },
   "source": [
    "### Get Top words and Trim Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a6EXs82eK4Oj"
   },
   "source": [
    "Basically, implement this SQL query in Pandas:\n",
    "```\n",
    "SELECT * \n",
    "FROM vocab \n",
    "WHERE stop = 0\n",
    "ORDER BY n DESC\n",
    "LIMIT 1000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a6EXs82eK4Oj"
   },
   "outputs": [],
   "source": [
    "def get_top_terms(vocab, no_stops=True, sort_col='n', k=1000):\n",
    "    if no_stops:\n",
    "        V = vocab[vocab.stop == 0]\n",
    "    else:\n",
    "        V = vocab\n",
    "    return V.sort_values(sort_col, ascending=False).head(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a6EXs82eK4Oj"
   },
   "source": [
    "### Remove proper nouns\n",
    "\n",
    "These make it too easy to distinguish genres, as they have super high TFIDF values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a6EXs82eK4Oj"
   },
   "outputs": [],
   "source": [
    "proper_nouns = tokens.loc[tokens.pos == 'NNP', 'term_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a6EXs82eK4Oj"
   },
   "outputs": [],
   "source": [
    "top_n = 1000\n",
    "# TOPV = get_top_terms(vocab, sort_col='n')\n",
    "TOPV = get_top_terms(vocab.loc[~vocab.index.isin(proper_nouns)], sort_col='n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Reduced TFIDF matrix for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_small = TFIDF[TOPV.index].stack().to_frame().rename(columns={0:'w'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8CCiKRQP4gWM",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eVQ1HNSn7P5J"
   },
   "outputs": [],
   "source": [
    "with sqlite3.connect(db_name) as db:\n",
    "    vocab.to_sql('vocab', db, if_exists='replace', index=True)\n",
    "    tokens.to_sql('token', db, if_exists='replace', index=True)\n",
    "    docs.to_sql('doc', db, if_exists='replace', index=True)\n",
    "    tfidf_small.to_sql('tfidf_small', db, if_exists='replace', index=True)\n",
    "    bags.reset_index().to_sql('bag', db, if_exists='replace', index=True, index_label='bag_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E3BY34SP68yR"
   },
   "outputs": [],
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TextmanVectorSpaceMaker.ipynb",
   "provenance": [
    {
     "file_id": "1UJXtZFtWykmkbZSLyLxpKmwiGhXr1w6P",
     "timestamp": 1550268040004
    }
   ],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
