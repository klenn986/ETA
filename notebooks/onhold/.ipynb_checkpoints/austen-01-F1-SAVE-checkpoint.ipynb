{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/rca2t/Public/ETA/lib')\n",
    "# import textman as tx\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/rca2t/Public/ETA/data/austen'\n",
    "OHCO = ['title_id', 'chap_num', 'para_num', 'token_num']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create corpus of documents from source docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import downloadd Gutenberg texts from source directory\n",
    "texts = []\n",
    "titles = []\n",
    "for path in glob.glob(data_dir + \"/*.txt\"):\n",
    "    \n",
    "    # Get Gutenberg id -- CHANGE TO REFLECT NEW FORMAT\n",
    "    guten_id = path.split('/')[-1].split('.')[0]\n",
    "        \n",
    "    # Import file into a dataframe\n",
    "    epub = open(path, 'r', encoding='utf-8-sig').readlines()\n",
    "    df = pd.DataFrame(epub, columns=['line_str'])\n",
    "    df.index.name = 'line_num'\n",
    "    df.line_str = df.line_str.str.strip()\n",
    "    del(epub)\n",
    "        \n",
    "    # Extract title of work from first line\n",
    "    title = df.loc[0].line_str.replace('The Project Gutenberg EBook of ', '')\n",
    "    titles.append((guten_id, title))\n",
    "    title_id = len(titles) - 1\n",
    "    print(guten_id, title_id, title)\n",
    "    \n",
    "    # Remove Gutenberg's front and back matter\n",
    "    a = df.line_str.str.match(r\"\\*\\*\\*\\s*START OF (THE|THIS) PROJECT\")\n",
    "    b = df.line_str.str.match(r\"\\*\\*\\*\\s*END OF (THE|THIS) PROJECT\")\n",
    "    an = df.loc[a].index[0]\n",
    "    bn = df.loc[b].index[0]\n",
    "    df = df.iloc[an + 1 : bn - 2]\n",
    "    \n",
    "    # Chunk by chapter\n",
    "    chap_lines = df.line_str.str.match(r\"\\s*chapter\", case=False)\n",
    "    chap_nums = [i+1 for i in range(df.loc[chap_lines].shape[0])]\n",
    "    df.loc[chap_lines, 'chap_num'] = chap_nums\n",
    "    df.chap_num = df.chap_num.ffill()\n",
    "    df = df.loc[~df.chap_num.isna()] # Remove chapter heading lines\n",
    "    df = df.loc[~chap_lines] # Remove everything before Chapter 1\n",
    "    df.chap_num = df.chap_num.astype('int') # Convert chap_num from float to int\n",
    "    df = df.groupby(['chap_num']).line_str.apply(lambda x: '\\n'.join(x)).to_frame() # Make big string\n",
    "    \n",
    "    # Gather paragraphs \n",
    "    df = df['line_str'].str.split(r'\\n\\n+', expand=True).stack()\\\n",
    "        .to_frame().rename(columns={0:'para_str'})\n",
    "    df.index.names = OHCO[1:3] # ['chap_num', 'para_num']\n",
    "    df['para_str'] = df['para_str'].str.replace(r'\\n', ' ').str.strip()\n",
    "    df = df[~df['para_str'].str.match(r'^\\s*$')] # Remove empty paragraphs\n",
    "    df['title_id'] = title_id\n",
    "    \n",
    "    # Add OHCO index\n",
    "    df = df.reset_index().set_index(OHCO[:3])\n",
    "\n",
    "    # Add to corpus\n",
    "    texts.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all texts into a corpus\n",
    "corpus = pd.concat(texts)\n",
    "del(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create library table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library = pd.DataFrame(titles, columns=['guten_id', 'gut_str'])\n",
    "library.index.name  = 'work_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library.gut_str = library.gut_str.str.replace('The Project Gutenberg eBook, ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library[['title', 'author']] = library['gut_str'].str.split(', by', expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.iloc[0].para_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create tokens table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = corpus['para_str'].str.split(r'\\W+', expand=True).stack().to_frame()\\\n",
    "        .rename(columns={0:'token_str'})\n",
    "tokens.index.names = OHCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# puncs = corpus['para_str'].str.split(r'\\w+', expand=True).stack().to_frame()\\\n",
    "#         .rename(columns={0:'punc'})\n",
    "# puncs.index.names = OHCO\n",
    "# pd.concat([tokens, puncs], 1)\n",
    "# puncs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens['term_str'] = tokens.token_str.str.lower().str.replace(r'_+', ' ').str.replace(r'\\s+', ' ').str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokens[tokens.term_str != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create vocabulary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokens.term_str.value_counts().to_frame().reset_index()\\\n",
    "    .rename(columns={'term_str':'term_count', 'index':'term_str'})\n",
    "vocab = vocab.sort_values('term_str').reset_index(drop=True)\n",
    "vocab.index.name = 'term_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.sort_values('term_count').tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.sort_values('term_count', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab[['term_str','term_count']].sort_values('term_count').tail(50)\\\n",
    "    .plot(kind='barh', figsize=(5,15), x='term_str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace `term_str` with `term_id` in tokens table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens['term_id'] = tokens.term_str.map(vocab.reset_index().set_index('term_str').term_id)\n",
    "# tokens = tokens.drop('term_str', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creat Doc-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK = OHCO[:3] # By Paragraph (could be Chap, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = tokens.groupby(CHUNK + ['term_id']).term_id.count()\\\n",
    "    .to_frame().rename(columns={'term_id':'n'}).unstack()\n",
    "dtm.columns = dtm.columns.droplevel(0)\n",
    "dtm = dtm.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm.sum().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add some stats to vocab table\n",
    "\n",
    "Here we take note of various statistics we learn from the tokens table and apply them to the vocab table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab['tf'] = vocab['term_count'] / vocab['term_count'].sum()\n",
    "vocab['std'] = dtm.std()\n",
    "vocab['mean'] = dtm.mean()\n",
    "vocab['max'] = dtm.max()\n",
    "vocab['df'] = dtm[dtm > 0].count()\n",
    "vocab['idf'] = np.log10(corpus.shape[0] / vocab.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "vocab['entropy'] = (dtm / dtm.sum()).apply(entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab[vocab.term_count >= 10].sort_values('idf', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.sort_values('entropy', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.plot(kind='scatter', x='idf', y='entropy', figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r, p, stderr = linregress(vocab.idf, vocab.entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab['test'] = (-1 * vocab.entropy) + intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.plot(kind='scatter', x='idf', y='test', figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.plot(kind='scatter', x='idf', y='entropy', figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab['len'] = vocab.term_str.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.sort_values('len', ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab['len'].value_counts().sort_index().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab['len'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library.to_csv(data_dir + '/austen-library.csv', )\n",
    "corpus.to_csv(data_dir + '/austen-corpus.csv')\n",
    "tokens.to_csv(data_dir + '/austen-tokens.csv')\n",
    "vocab.to_csv(data_dir + '/austen-vocab.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "with sqlite3.connect(data_dir + '/austen.db') as db:\n",
    "    library.to_sql('library', db, index=True, if_exists='replace')\n",
    "    corpus.to_sql('corpus', db, index=True, if_exists='replace')\n",
    "    tokens.to_sql('tokens', db, index=True, if_exists='replace')\n",
    "    vocab.to_sql('vocab', db, index=True, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm.sum(1).plot.kde()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md",
   "text_representation": {
    "extension": ".md",
    "format_name": "markdown",
    "format_version": "1.1",
    "jupytext_version": "1.1.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
