{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4: NLP and the Pipeline\n",
    "\n",
    "* DS 6001\n",
    "* Raf Alvarado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ai-gvPnADykO",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Overview\n",
    "\n",
    "We import a collection of texts and convert to F2. Then we annotate the collection to create an F3-level model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MwrVU8kZDykb"
   },
   "outputs": [],
   "source": [
    "OHCO = ['book_id', 'chap_num', 'para_num', 'sent_num', 'token_num']\n",
    "epub_dir = 'epubs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MwrVU8kZDykb"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MwrVU8kZDykb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MwrVU8kZDykb"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CAzRbscg1ziu"
   },
   "source": [
    "## Import NLTK and download resources\n",
    "\n",
    "If you need to install NLTK, see the [instructions here](https://www.nltk.org/install.html). You can also install this with Anaconda, like so:\n",
    "\n",
    "`conda install nltk`\n",
    "\n",
    "Once you have installed NLTK, you will need to download resources, which will happen when you run the following cell. If the interactive window opens, you may need to set your NLTK Data Directory, as described in the [instructions here](https://www.nltk.org/data.html). To set the directory, click on the File menu and select Change Download Directory. For central installation, set this to `C:\\nltk_data` (Windows),`/usr/local/share/nltk_data` (Mac), or `/usr/share/nltk_data` (Unix). \n",
    "\n",
    "> If you did not install the data to one of the above central locations, you will need to set the NLTK_DATA environment variable to specify the location of the data. (On a Windows machine, right click on “My Computer” then select Properties > Advanced > Environment Variables > User Variables > New...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "colab_type": "code",
    "id": "Q4Id4bNP5t4p",
    "outputId": "1be778b1-a6a6-45e5-8773-010e83904be3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 8] nodename nor\n",
      "[nltk_data]     servname provided, or not known>\n",
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [Errno 8] nodename nor servname provided, or not\n",
      "[nltk_data]     known>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 8] nodename\n",
      "[nltk_data]     nor servname provided, or not known>\n",
      "[nltk_data] Error loading tagsets: <urlopen error [Errno 8] nodename\n",
      "[nltk_data]     nor servname provided, or not known>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquire\n",
    "\n",
    "We download the novels of Jane Austen and Herman Melville from Project Gutenberg. I actually used a utility I created called PGTK:\n",
    "\n",
    "* https://github.com/ontoligent-design/pgtk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect\n",
    "\n",
    "Since Project Gutenberg texts vary widely in their markup, we define our chunking patterns by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "roman = '[IVXLCM]+'\n",
    "caps = \"[A-Z';, -]+\"\n",
    "chap_pats = {\n",
    "    158: {\n",
    "        'start_line': 37,\n",
    "        'end_line': 16261,\n",
    "        'volume': re.compile('^\\s*VOLUME\\s+{}\\s*$'.format(roman)),\n",
    "        'chapter': re.compile('^\\s*CHAPTER\\s+{}\\s*$'.format(roman))\n",
    "    },\n",
    "    946: {\n",
    "        'start_line': 38,\n",
    "        'end_line': 2556,\n",
    "        'chapter': re.compile(\"^\\s*{}\\s*$\".format(roman))\n",
    "    },\n",
    "    1212: {\n",
    "        'start_line': 77,\n",
    "        'end_line': 3432,\n",
    "        'chapter': re.compile(\"^\\s*LETTER .* to .*$\")\n",
    "    },\n",
    "    141: {\\\n",
    "        'start_line': 40,\n",
    "        'end_line': 15376,\n",
    "        'chapter': re.compile(\"^CHAPTER\\s+{}$\".format(roman))\n",
    "    },\n",
    "    121: {\n",
    "        'start_line': 57,\n",
    "        'end_line': 7874,\n",
    "        'chapter': re.compile(\"^CHAPTER\\s+\\d+$\")\n",
    "    },\n",
    "    105: {\n",
    "        'start_line': 48,\n",
    "        'end_line': 8360,\n",
    "        'chapter': re.compile(\"^Chapter\\s+\\d+$\")\n",
    "    },\n",
    "    1342: {\n",
    "        'start_line': 37,\n",
    "        'end_line': 13061,\n",
    "        'chapter': re.compile(\"^Chapter\\s+\\d+$\")\n",
    "    },\n",
    "    161: {\n",
    "        'start_line': 43,\n",
    "        'end_line': 12654,\n",
    "        'chapter': re.compile(\"^CHAPTER\\s+\\d+$\")          \n",
    "    },\n",
    "    15422: {\n",
    "        'start_line': 193,\n",
    "        'end_line': 7501,\n",
    "        'chapter': re.compile(\"^\\s*CHAPTER\\s+{}\\.\".format(roman))\n",
    "    },\n",
    "    13720: {\n",
    "        'start_line': 187,\n",
    "        'end_line': 11470,\n",
    "        'chapter': re.compile(\"^\\s*CHAPTER\\s+{}\\s*$\".format(roman))\n",
    "    },\n",
    "    13721: {\n",
    "        'start_line': 164,\n",
    "        'end_line': 13135,\n",
    "        'chapter': re.compile(\"^\\s*CHAPTER\\s+{}\\s*$\".format(roman))\n",
    "    },\n",
    "    2701: {\n",
    "        'start_line': 52,\n",
    "        'end_line': 21743,\n",
    "        'chapter': re.compile(\"^(ETYMOLOGY|EXTRACTS|CHAPTER)\")\n",
    "    },\n",
    "    4045: {\n",
    "        'start_line': 138,\n",
    "        'end_line': 11655,\n",
    "        'volume': re.compile(\"^\\s*PART\\s+{}\\s*$\".format(roman)),\n",
    "        'chapter': re.compile(\"^\\s*CHAPTER\\s+{}\\.\\s*$\".format(roman))\n",
    "    },\n",
    "    34970: {\n",
    "        'start_line': 234,\n",
    "        'end_line': 16267,\n",
    "        'volume': re.compile(\"^\\s*BOOK\\s+{}\\.\\s*$\".format(roman)),\n",
    "        'chapter': re.compile(\"^\\s*{}\\.\\s*$\".format(roman))\n",
    "    },\n",
    "    8118: {\n",
    "        'start_line': 142,\n",
    "        'end_line': 12300,\n",
    "        'chapter': re.compile(\"^\\s*{}\\. .*$\".format(roman))\n",
    "    },\n",
    "    53861: {\n",
    "        'start_line': 129,\n",
    "        'end_line': 6904,\n",
    "        'chapter': re.compile('^\\s*{}\\s*$'.format(caps))\n",
    "    },\n",
    "    21816: {\n",
    "        'start_line': 309,\n",
    "        'end_line': 11023,\n",
    "        'chapter': re.compile('^CHAPTER\\s+{}\\.?$'.format(roman))\n",
    "    },\n",
    "    15859: {\n",
    "        'start_line': 77,\n",
    "        'end_line': 8619,\n",
    "        'chapter': re.compile('^\\s*{}\\s*$'.format(caps))\n",
    "    },\n",
    "    1900: {\n",
    "        'start_line': 43,\n",
    "        'end_line': 11216,\n",
    "        'chapter': re.compile(\"^CHAPTER\\s+\\w+\\s*$\")\n",
    "    },\n",
    "    10712: {\n",
    "        'start_line': 205,\n",
    "        'end_line': 15487,\n",
    "        'chapter': re.compile(\"^CHAPTER\\s+{}\\.\\s*$\".format(roman))\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register and Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquire_epubs(epub_list, chap_pats, OHCO=OHCO):\n",
    "    \n",
    "    my_lib = []\n",
    "    my_doc = []\n",
    "\n",
    "    for epub_file in epub_list:\n",
    "        \n",
    "        # Get PG ID from filename\n",
    "        book_id = int(epub_file.split('-')[-1].split('.')[0].replace('pg',''))\n",
    "        print(\"BOOK ID\", book_id)\n",
    "        \n",
    "        # Import file as lines\n",
    "        lines = open(epub_file, 'r', encoding='utf-8-sig').readlines()\n",
    "        df = pd.DataFrame(lines, columns=['line_str'])\n",
    "        df.index.name = 'line_num'\n",
    "        df.line_str = df.line_str.str.strip()\n",
    "        df['book_id'] = book_id\n",
    "        \n",
    "        # FIX CHARACTERS TO IMPROVE TOKENIZATION\n",
    "        df.line_str = df.line_str.str.replace('—', ' — ')\n",
    "        df.line_str = df.line_str.str.replace('-', ' - ')\n",
    "        \n",
    "        # Get book title and put into LIB table -- note problems, though\n",
    "        book_title = re.sub(r\"The Project Gutenberg eBook( of|,) \", \"\", df.loc[0].line_str, flags=re.IGNORECASE)\n",
    "        book_title = re.sub(r\"Project Gutenberg's \", \"\", book_title, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Remove cruft\n",
    "        a = chap_pats[book_id]['start_line'] - 1\n",
    "        b = chap_pats[book_id]['end_line'] + 1\n",
    "        df = df.iloc[a:b]\n",
    "        \n",
    "        # Chunk by chapter\n",
    "        chap_lines = df.line_str.str.match(chap_pats[book_id]['chapter'])\n",
    "        chap_nums = [i+1 for i in range(df.loc[chap_lines].shape[0])]\n",
    "        df.loc[chap_lines, 'chap_num'] = chap_nums\n",
    "        df.chap_num = df.chap_num.ffill()\n",
    "\n",
    "        # Clean up\n",
    "        df = df[~df.chap_num.isna()] # Remove chapter heading lines\n",
    "        df = df.loc[~chap_lines] # Remove everything before Chapter 1\n",
    "        df['chap_num'] = df['chap_num'].astype('int')\n",
    "        \n",
    "        # Group -- Note that we exclude the book level in the OHCO at this point\n",
    "        df = df.groupby(OHCO[1:2]).line_str.apply(lambda x: '\\n'.join(x)).to_frame() # Make big string\n",
    "        \n",
    "        # Split into paragrpahs\n",
    "        df = df['line_str'].str.split(r'\\n\\n+', expand=True).stack().to_frame().rename(columns={0:'para_str'})\n",
    "        df.index.names = OHCO[1:3] # MAY NOT BE NECESSARY UNTIL THE END\n",
    "        df['para_str'] = df['para_str'].str.replace(r'\\n', ' ').str.strip()\n",
    "        df = df[~df['para_str'].str.match(r'^\\s*$')] # Remove empty paragraphs\n",
    "        \n",
    "        # Set index\n",
    "        df['book_id'] = book_id\n",
    "        df = df.reset_index().set_index(OHCO[:3])\n",
    "\n",
    "        # Register\n",
    "        my_lib.append((book_id, book_title, epub_file))\n",
    "        my_doc.append(df)\n",
    "\n",
    "    docs = pd.concat(my_doc)\n",
    "    library = pd.DataFrame(my_lib, columns=['book_id', 'book_title', 'book_file']).set_index('book_id')\n",
    "    return library, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOOK ID 158\n",
      "BOOK ID 946\n",
      "BOOK ID 1212\n",
      "BOOK ID 141\n",
      "BOOK ID 121\n",
      "BOOK ID 105\n",
      "BOOK ID 1342\n",
      "BOOK ID 161\n",
      "BOOK ID 15422\n",
      "BOOK ID 13720\n",
      "BOOK ID 13721\n",
      "BOOK ID 2701\n",
      "BOOK ID 4045\n",
      "BOOK ID 34970\n",
      "BOOK ID 8118\n",
      "BOOK ID 53861\n",
      "BOOK ID 21816\n",
      "BOOK ID 15859\n",
      "BOOK ID 1900\n",
      "BOOK ID 10712\n"
     ]
    }
   ],
   "source": [
    "epubs = [epub for epub in sorted(glob(epub_dir+'/*.txt'))]\n",
    "LIB, DOC = acquire_epubs(epubs, chap_pats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>para_str</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th>chap_num</th>\n",
       "      <th>para_num</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13721</th>\n",
       "      <th>28</th>\n",
       "      <th>17</th>\n",
       "      <td>\"Peace! everlasting foes,\" cried Media, interp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21816</th>\n",
       "      <th>25</th>\n",
       "      <th>3</th>\n",
       "      <td>It was in the semicircular porch of a cabin, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342</th>\n",
       "      <th>43</th>\n",
       "      <th>71</th>\n",
       "      <td>Elizabeth excused herself as well as she could...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2701</th>\n",
       "      <th>122</th>\n",
       "      <th>26</th>\n",
       "      <td>The tableau all waned at last with the pallidn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21816</th>\n",
       "      <th>2</th>\n",
       "      <th>29</th>\n",
       "      <td>As pine, beech, birch, ash, hackmatack, hemloc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4045</th>\n",
       "      <th>76</th>\n",
       "      <th>1</th>\n",
       "      <td>UPON arriving home we fully laid open to Po - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2701</th>\n",
       "      <th>90</th>\n",
       "      <th>15</th>\n",
       "      <td>Stripped to our shirts and drawers, we sprang ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10712</th>\n",
       "      <th>20</th>\n",
       "      <th>6</th>\n",
       "      <td>During warm nights in the Tropics, your hammoc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34970</th>\n",
       "      <th>50</th>\n",
       "      <th>10</th>\n",
       "      <td>But her gentler sex returned to Isabel at last...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13721</th>\n",
       "      <th>32</th>\n",
       "      <th>39</th>\n",
       "      <td>\"Shall I adjourn the court then, my lord?\" sai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    para_str\n",
       "book_id chap_num para_num                                                   \n",
       "13721   28       17        \"Peace! everlasting foes,\" cried Media, interp...\n",
       "21816   25       3         It was in the semicircular porch of a cabin, o...\n",
       "1342    43       71        Elizabeth excused herself as well as she could...\n",
       "2701    122      26        The tableau all waned at last with the pallidn...\n",
       "21816   2        29        As pine, beech, birch, ash, hackmatack, hemloc...\n",
       "4045    76       1         UPON arriving home we fully laid open to Po - ...\n",
       "2701    90       15        Stripped to our shirts and drawers, we sprang ...\n",
       "10712   20       6         During warm nights in the Tropics, your hammoc...\n",
       "34970   50       10        But her gentler sex returned to Isabel at last...\n",
       "13721   32       39        \"Shall I adjourn the court then, my lord?\" sai..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DOC.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize\n",
    "\n",
    "We use NLTK this time. Note that this process takes some time, mainly because the NLTK functions are not optimized for dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc_df, remove_pos_tuple=False, OHCO=OHCO):\n",
    "    \n",
    "    # Paragraphs to Sentences\n",
    "    df = doc_df.para_str\\\n",
    "        .apply(lambda x: pd.Series(nltk.sent_tokenize(x)))\\\n",
    "        .stack()\\\n",
    "        .to_frame()\\\n",
    "        .rename(columns={0:'sent_str'})\n",
    "    \n",
    "    # Sentences to Tokens\n",
    "    # .apply(lambda x: pd.Series(nltk.pos_tag(nltk.word_tokenize(x))))\\\n",
    "    df = df.sent_str\\\n",
    "        .apply(lambda x: pd.Series(nltk.pos_tag(nltk.WhitespaceTokenizer().tokenize(x))))\\\n",
    "        .stack()\\\n",
    "        .to_frame()\\\n",
    "        .rename(columns={0:'pos_tuple'})\n",
    "    \n",
    "    # Grab info from tuple\n",
    "    df['pos'] = df.pos_tuple.apply(lambda x: x[1])\n",
    "    df['token_str'] = df.pos_tuple.apply(lambda x: x[0])\n",
    "    if remove_pos_tuple:\n",
    "        df = df.drop('pos_tuple', 1)\n",
    "    \n",
    "    # Add index\n",
    "    df.index.names = OHCO\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = tokenize(DOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>pos_tuple</th>\n",
       "      <th>pos</th>\n",
       "      <th>token_str</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th>chap_num</th>\n",
       "      <th>para_num</th>\n",
       "      <th>sent_num</th>\n",
       "      <th>token_num</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">158</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>(Emma, NNP)</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Emma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Woodhouse,, NNP)</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Woodhouse,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(handsome,, NN)</td>\n",
       "      <td>NN</td>\n",
       "      <td>handsome,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(clever,, NN)</td>\n",
       "      <td>NN</td>\n",
       "      <td>clever,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(and, CC)</td>\n",
       "      <td>CC</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      pos_tuple  pos  \\\n",
       "book_id chap_num para_num sent_num token_num                           \n",
       "158     1        1        0        0                (Emma, NNP)  NNP   \n",
       "                                   1          (Woodhouse,, NNP)  NNP   \n",
       "                                   2            (handsome,, NN)   NN   \n",
       "                                   3              (clever,, NN)   NN   \n",
       "                                   4                  (and, CC)   CC   \n",
       "\n",
       "                                               token_str  \n",
       "book_id chap_num para_num sent_num token_num              \n",
       "158     1        1        0        0                Emma  \n",
       "                                   1          Woodhouse,  \n",
       "                                   2           handsome,  \n",
       "                                   3             clever,  \n",
       "                                   4                 and  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKEN.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>pos_tuple</th>\n",
       "      <th>pos</th>\n",
       "      <th>token_str</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th>chap_num</th>\n",
       "      <th>para_num</th>\n",
       "      <th>sent_num</th>\n",
       "      <th>token_num</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"30\" valign=\"top\">158</th>\n",
       "      <th rowspan=\"30\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>8</th>\n",
       "      <td>(comfortable, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>comfortable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(happy, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(best, JJS)</td>\n",
       "      <td>JJS</td>\n",
       "      <td>best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>(twenty, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>twenty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>(little, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>little</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">2</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">0</th>\n",
       "      <th>3</th>\n",
       "      <td>(youngest, JJS)</td>\n",
       "      <td>JJS</td>\n",
       "      <td>youngest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(affectionate,, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>affectionate,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>(early, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>early</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>11</th>\n",
       "      <td>(more, JJR)</td>\n",
       "      <td>JJR</td>\n",
       "      <td>more</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(indistinct, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>indistinct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>(excellent, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>excellent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>(little, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>little</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>(short, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">3</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>(Sixteen, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>Sixteen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(friend,, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>friend,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>9</th>\n",
       "      <td>(nominal, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>nominal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>(attached,, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>attached,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">4</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">0</th>\n",
       "      <th>1</th>\n",
       "      <td>(real, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(evils,, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>evils,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(much, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>much</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(own, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>(little, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>little</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>(many, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>many</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>5</th>\n",
       "      <td>(present, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(unperceived,, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>unperceived,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">5</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>5</th>\n",
       "      <td>(gentle, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>gentle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(disagreeable, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>disagreeable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>16</th>\n",
       "      <td>(mournful, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>mournful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th>5</th>\n",
       "      <td>(bride, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>bride</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(third, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>third</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"30\" valign=\"top\">10712</th>\n",
       "      <th rowspan=\"30\" valign=\"top\">92</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">17</th>\n",
       "      <th>0</th>\n",
       "      <th>15</th>\n",
       "      <td>(unhappy,, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>unhappy,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>6</th>\n",
       "      <td>(gallant, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>gallant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(top, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">18</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>10</th>\n",
       "      <td>(main, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>main</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>(own, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>21</th>\n",
       "      <td>(unlegalised, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>unlegalised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">19</th>\n",
       "      <th>0</th>\n",
       "      <th>27</th>\n",
       "      <td>(little, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>little</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">1</th>\n",
       "      <th>17</th>\n",
       "      <td>(clean, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>clean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(oft, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>oft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>(painted, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>painted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>(vast, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>vast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">20</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">0</th>\n",
       "      <th>8</th>\n",
       "      <td>(up,, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>up,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>(more, JJR)</td>\n",
       "      <td>JJR</td>\n",
       "      <td>more</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>(tow, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>tow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>(own, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <th>0</th>\n",
       "      <th>15</th>\n",
       "      <td>(subterranean, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>subterranean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">22</th>\n",
       "      <th>1</th>\n",
       "      <th>4</th>\n",
       "      <td>(many, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>many</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>5</th>\n",
       "      <td>(full, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>full</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>25</th>\n",
       "      <td>(indefinite, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>indefinite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>2</th>\n",
       "      <td>(worst, JJS)</td>\n",
       "      <td>JJS</td>\n",
       "      <td>worst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">5</th>\n",
       "      <th>2</th>\n",
       "      <td>(last, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>last</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(another;, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>another;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(own, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">6</th>\n",
       "      <th>5</th>\n",
       "      <td>(us,, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>us,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(murderous, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>murderous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(bloody, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>bloody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">7</th>\n",
       "      <th>9</th>\n",
       "      <td>(long, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(unredressed,, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>unredressed,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">23</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>2</th>\n",
       "      <td>(us,, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>us,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(surround,, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>surround,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>166430 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        pos_tuple  pos  \\\n",
       "book_id chap_num para_num sent_num token_num                             \n",
       "158     1        1        0        8            (comfortable, JJ)   JJ   \n",
       "                                   11                 (happy, JJ)   JJ   \n",
       "                                   19                 (best, JJS)  JJS   \n",
       "                                   27                (twenty, JJ)   JJ   \n",
       "                                   36                (little, JJ)   JJ   \n",
       "                 2        0        3              (youngest, JJS)  JJS   \n",
       "                                   11         (affectionate,, JJ)   JJ   \n",
       "                                   30                 (early, JJ)   JJ   \n",
       "                          1        11                 (more, JJR)  JJR   \n",
       "                                   14            (indistinct, JJ)   JJ   \n",
       "                                   27             (excellent, JJ)   JJ   \n",
       "                                   34                (little, JJ)   JJ   \n",
       "                                   35                 (short, JJ)   JJ   \n",
       "                 3        0        0                (Sixteen, JJ)   JJ   \n",
       "                                   16               (friend,, JJ)   JJ   \n",
       "                          2        9                (nominal, JJ)   JJ   \n",
       "                                   47             (attached,, JJ)   JJ   \n",
       "                 4        0        1                   (real, JJ)   JJ   \n",
       "                                   2                 (evils,, JJ)   JJ   \n",
       "                                   14                  (much, JJ)   JJ   \n",
       "                                   16                   (own, JJ)   JJ   \n",
       "                                   24                (little, JJ)   JJ   \n",
       "                                   38                  (many, JJ)   JJ   \n",
       "                          1        5                (present, JJ)   JJ   \n",
       "                                   7           (unperceived,, JJ)   JJ   \n",
       "                 5        0        5                 (gentle, JJ)   JJ   \n",
       "                                   18          (disagreeable, JJ)   JJ   \n",
       "                          3        16              (mournful, JJ)   JJ   \n",
       "                          4        5                  (bride, JJ)   JJ   \n",
       "                                   23                 (third, JJ)   JJ   \n",
       "...                                                           ...  ...   \n",
       "10712   92       17       0        15              (unhappy,, JJ)   JJ   \n",
       "                          2        6                (gallant, JJ)   JJ   \n",
       "                                   11                   (top, JJ)   JJ   \n",
       "                 18       0        10                  (main, JJ)   JJ   \n",
       "                                   34                   (own, JJ)   JJ   \n",
       "                          1        21           (unlegalised, JJ)   JJ   \n",
       "                 19       0        27                (little, JJ)   JJ   \n",
       "                          1        17                 (clean, JJ)   JJ   \n",
       "                                   22                   (oft, JJ)   JJ   \n",
       "                                   24               (painted, JJ)   JJ   \n",
       "                                   32                  (vast, JJ)   JJ   \n",
       "                 20       0        8                    (up,, JJ)   JJ   \n",
       "                                   21                 (more, JJR)  JJR   \n",
       "                                   35                   (tow, JJ)   JJ   \n",
       "                                   40                   (own, JJ)   JJ   \n",
       "                 21       0        15          (subterranean, JJ)   JJ   \n",
       "                 22       1        4                   (many, JJ)   JJ   \n",
       "                          2        5                   (full, JJ)   JJ   \n",
       "                          3        25            (indefinite, JJ)   JJ   \n",
       "                          4        2                 (worst, JJS)  JJS   \n",
       "                          5        2                   (last, JJ)   JJ   \n",
       "                                   8               (another;, JJ)   JJ   \n",
       "                                   15                   (own, JJ)   JJ   \n",
       "                          6        5                    (us,, JJ)   JJ   \n",
       "                                   11             (murderous, JJ)   JJ   \n",
       "                                   19                (bloody, JJ)   JJ   \n",
       "                          7        9                   (long, JJ)   JJ   \n",
       "                                   17          (unredressed,, JJ)   JJ   \n",
       "                 23       0        2                    (us,, JJ)   JJ   \n",
       "                                   4              (surround,, JJ)   JJ   \n",
       "\n",
       "                                                  token_str  \n",
       "book_id chap_num para_num sent_num token_num                 \n",
       "158     1        1        0        8            comfortable  \n",
       "                                   11                 happy  \n",
       "                                   19                  best  \n",
       "                                   27                twenty  \n",
       "                                   36                little  \n",
       "                 2        0        3               youngest  \n",
       "                                   11         affectionate,  \n",
       "                                   30                 early  \n",
       "                          1        11                  more  \n",
       "                                   14            indistinct  \n",
       "                                   27             excellent  \n",
       "                                   34                little  \n",
       "                                   35                 short  \n",
       "                 3        0        0                Sixteen  \n",
       "                                   16               friend,  \n",
       "                          2        9                nominal  \n",
       "                                   47             attached,  \n",
       "                 4        0        1                   real  \n",
       "                                   2                 evils,  \n",
       "                                   14                  much  \n",
       "                                   16                   own  \n",
       "                                   24                little  \n",
       "                                   38                  many  \n",
       "                          1        5                present  \n",
       "                                   7           unperceived,  \n",
       "                 5        0        5                 gentle  \n",
       "                                   18          disagreeable  \n",
       "                          3        16              mournful  \n",
       "                          4        5                  bride  \n",
       "                                   23                 third  \n",
       "...                                                     ...  \n",
       "10712   92       17       0        15              unhappy,  \n",
       "                          2        6                gallant  \n",
       "                                   11                   top  \n",
       "                 18       0        10                  main  \n",
       "                                   34                   own  \n",
       "                          1        21           unlegalised  \n",
       "                 19       0        27                little  \n",
       "                          1        17                 clean  \n",
       "                                   22                   oft  \n",
       "                                   24               painted  \n",
       "                                   32                  vast  \n",
       "                 20       0        8                    up,  \n",
       "                                   21                  more  \n",
       "                                   35                   tow  \n",
       "                                   40                   own  \n",
       "                 21       0        15          subterranean  \n",
       "                 22       1        4                   many  \n",
       "                          2        5                   full  \n",
       "                          3        25            indefinite  \n",
       "                          4        2                  worst  \n",
       "                          5        2                   last  \n",
       "                                   8               another;  \n",
       "                                   15                   own  \n",
       "                          6        5                    us,  \n",
       "                                   11             murderous  \n",
       "                                   19                bloody  \n",
       "                          7        9                   long  \n",
       "                                   17          unredressed,  \n",
       "                 23       0        2                    us,  \n",
       "                                   4              surround,  \n",
       "\n",
       "[166430 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKEN[TOKEN.pos.str.match('^JJ')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Reduce\n",
    "\n",
    "Extract a vocabulary from the TOKEN table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN['term_str'] = TOKEN['token_str'].str.lower().str.replace('[\\W_]', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = TOKEN.term_str.value_counts().to_frame().rename(columns={'index':'term_str', 'term_str':'n'})\\\n",
    "    .sort_index().reset_index().rename(columns={'index':'term_str'})\n",
    "VOCAB.index.name = 'term_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "colab_type": "code",
    "id": "3iYsuby6FYYR",
    "outputId": "b29dd1cd-a82a-44be-e21c-aa9b7e1e4933"
   },
   "outputs": [],
   "source": [
    "VOCAB['num'] = VOCAB.term_str.str.match(\"\\d+\").astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "colab_type": "code",
    "id": "3iYsuby6FYYR",
    "outputId": "b29dd1cd-a82a-44be-e21c-aa9b7e1e4933"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term_str</th>\n",
       "      <th>n</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>50493</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10440</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10800</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10th</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>118952</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>11th</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>120000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>125000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>12th</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>13000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>139</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1399</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>13th</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>140</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>144000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1492</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>14th</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>150</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40451</th>\n",
       "      <td>zigzags</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40452</th>\n",
       "      <td>zimmermann</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40453</th>\n",
       "      <td>zion</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40454</th>\n",
       "      <td>zmiglandi</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40455</th>\n",
       "      <td>znobbi</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40456</th>\n",
       "      <td>znobbis</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40457</th>\n",
       "      <td>znorto</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40458</th>\n",
       "      <td>zodiac</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40459</th>\n",
       "      <td>zodiacs</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40460</th>\n",
       "      <td>zogranda</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40461</th>\n",
       "      <td>zone</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40462</th>\n",
       "      <td>zoned</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40463</th>\n",
       "      <td>zones</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40464</th>\n",
       "      <td>zono</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40465</th>\n",
       "      <td>zonoree</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40466</th>\n",
       "      <td>zoological</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40467</th>\n",
       "      <td>zoology</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40468</th>\n",
       "      <td>zooperbi</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40469</th>\n",
       "      <td>zoophytes</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40470</th>\n",
       "      <td>zophar</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40471</th>\n",
       "      <td>zoroaster</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40472</th>\n",
       "      <td>zozo</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40473</th>\n",
       "      <td>zuma</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40474</th>\n",
       "      <td>zur</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40475</th>\n",
       "      <td>à</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40476</th>\n",
       "      <td>æneas</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40477</th>\n",
       "      <td>æniad</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40478</th>\n",
       "      <td>æson</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40479</th>\n",
       "      <td>æsops</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40480</th>\n",
       "      <td>ł20000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40481 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           term_str      n  num\n",
       "term_id                        \n",
       "0                    50493    0\n",
       "1                 0      2    1\n",
       "2                 1     18    1\n",
       "3                10      6    1\n",
       "4               100      2    1\n",
       "5              1000      2    1\n",
       "6             10000      3    1\n",
       "7           1000000      1    1\n",
       "8          10000000      1    1\n",
       "9             10440      1    1\n",
       "10            10800      2    1\n",
       "11             10th      2    1\n",
       "12         11000000      1    1\n",
       "13           118952      1    1\n",
       "14             11th      2    1\n",
       "15               12      5    1\n",
       "16           120000      1    1\n",
       "17           125000      1    1\n",
       "18             12th      1    1\n",
       "19               13      1    1\n",
       "20            13000      1    1\n",
       "21              139      2    1\n",
       "22             1399      1    1\n",
       "23             13th      3    1\n",
       "24              140      1    1\n",
       "25           144000      1    1\n",
       "26             1492      2    1\n",
       "27             14th      3    1\n",
       "28               15      2    1\n",
       "29              150      1    1\n",
       "...             ...    ...  ...\n",
       "40451       zigzags      2    0\n",
       "40452    zimmermann      3    0\n",
       "40453          zion      1    0\n",
       "40454     zmiglandi      2    0\n",
       "40455        znobbi      3    0\n",
       "40456       znobbis      1    0\n",
       "40457        znorto      1    0\n",
       "40458        zodiac     13    0\n",
       "40459       zodiacs      1    0\n",
       "40460      zogranda      1    0\n",
       "40461          zone     20    0\n",
       "40462         zoned      7    0\n",
       "40463         zones     11    0\n",
       "40464          zono      3    0\n",
       "40465       zonoree      3    0\n",
       "40466    zoological      5    0\n",
       "40467       zoology      4    0\n",
       "40468      zooperbi      2    0\n",
       "40469     zoophytes      2    0\n",
       "40470        zophar      1    0\n",
       "40471     zoroaster      2    0\n",
       "40472          zozo      1    0\n",
       "40473          zuma      8    0\n",
       "40474           zur      2    0\n",
       "40475             à      5    0\n",
       "40476         æneas      1    0\n",
       "40477         æniad      1    0\n",
       "40478          æson      2    0\n",
       "40479         æsops      1    0\n",
       "40480        ł20000      1    0\n",
       "\n",
       "[40481 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bDSH9L2TXGzH",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Annotate (VOCAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "US7EfWK_06FS"
   },
   "source": [
    "## Add Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BDCfFuN80_rX"
   },
   "source": [
    "We use NLTK's built in stopword list for English. Note that we can add and subtract from this list, or just create our own list and keep it in our data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RG-5qYDR1YC2"
   },
   "outputs": [],
   "source": [
    "sw = pd.DataFrame(nltk.corpus.stopwords.words('english'), columns=['term_str'])\n",
    "sw = sw.reset_index().set_index('term_str')\n",
    "sw.columns = ['dummy']\n",
    "sw.dummy = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1925
    },
    "colab_type": "code",
    "id": "8vtGY9V82scz",
    "outputId": "e7ef30c7-3a05-4acf-e2cc-9154f589bd91"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dummy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term_str</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ourselves</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whom</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>more</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>at</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>once</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mightn</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>won</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>these</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shouldn't</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dummy\n",
       "term_str        \n",
       "ourselves      1\n",
       "whom           1\n",
       "more           1\n",
       "at             1\n",
       "once           1\n",
       "mightn         1\n",
       "won            1\n",
       "my             1\n",
       "these          1\n",
       "shouldn't      1"
      ]
     },
     "execution_count": 600,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cVJUOP9l2AS7"
   },
   "outputs": [],
   "source": [
    "VOCAB['stop'] = VOCAB.term_str.map(sw.dummy)\n",
    "VOCAB['stop'] = VOCAB['stop'].fillna(0).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "colab_type": "code",
    "id": "QXcA9xyY4JF_",
    "outputId": "340d1dab-1901-4eeb-b9d8-a269eba90dea"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term_str</th>\n",
       "      <th>n</th>\n",
       "      <th>num</th>\n",
       "      <th>stop</th>\n",
       "      <th>p_stem</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24083</th>\n",
       "      <td>now</td>\n",
       "      <td>5773</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16913</th>\n",
       "      <td>him</td>\n",
       "      <td>9194</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>him</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34853</th>\n",
       "      <td>t</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35475</th>\n",
       "      <td>these</td>\n",
       "      <td>2704</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>these</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23982</th>\n",
       "      <td>nor</td>\n",
       "      <td>1219</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>nor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24504</th>\n",
       "      <td>only</td>\n",
       "      <td>3398</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>onli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17310</th>\n",
       "      <td>how</td>\n",
       "      <td>3070</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>how</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16734</th>\n",
       "      <td>her</td>\n",
       "      <td>17020</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>her</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32652</th>\n",
       "      <td>so</td>\n",
       "      <td>9843</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10906</th>\n",
       "      <td>down</td>\n",
       "      <td>2451</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        term_str      n  num  stop p_stem\n",
       "term_id                                  \n",
       "24083        now   5773    0     1    now\n",
       "16913        him   9194    0     1    him\n",
       "34853          t     30    0     1      t\n",
       "35475      these   2704    0     1  these\n",
       "23982        nor   1219    0     1    nor\n",
       "24504       only   3398    0     1   onli\n",
       "17310        how   3070    0     1    how\n",
       "16734        her  17020    0     1    her\n",
       "32652         so   9843    0     1     so\n",
       "10906       down   2451    0     1   down"
      ]
     },
     "execution_count": 606,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB[VOCAB.stop == 1].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bDSH9L2TXGzH"
   },
   "source": [
    "## Add Stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mE_YGklKXSYn"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "VOCAB['p_stem'] = VOCAB.term_str.apply(stemmer.stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "colab_type": "code",
    "id": "dY__Bq0yXqbj",
    "outputId": "eddcdafe-e378-4f7b-ac6b-1fc41ef64fbb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term_str</th>\n",
       "      <th>n</th>\n",
       "      <th>num</th>\n",
       "      <th>stop</th>\n",
       "      <th>p_stem</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33307</th>\n",
       "      <td>sprig</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>sprig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38270</th>\n",
       "      <td>upbraids</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>upbraid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36640</th>\n",
       "      <td>truxills</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>truxil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2161</th>\n",
       "      <td>askance</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>askanc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35953</th>\n",
       "      <td>tolerantly</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>tolerantli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11835</th>\n",
       "      <td>emphatic</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>emphat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7224</th>\n",
       "      <td>concernments</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>concern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3925</th>\n",
       "      <td>blocks</td>\n",
       "      <td>42</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>block</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13099</th>\n",
       "      <td>fallen</td>\n",
       "      <td>102</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>fallen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37903</th>\n",
       "      <td>unrecorded</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>unrecord</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             term_str    n    num  stop      p_stem\n",
       "term_id                                            \n",
       "33307           sprig    1  False     0       sprig\n",
       "38270        upbraids    2  False     0     upbraid\n",
       "36640        truxills    1  False     0      truxil\n",
       "2161          askance    4  False     0      askanc\n",
       "35953      tolerantly    1  False     0  tolerantli\n",
       "11835        emphatic    6  False     0      emphat\n",
       "7224     concernments    2  False     0     concern\n",
       "3925           blocks   42  False     0       block\n",
       "13099          fallen  102  False     0      fallen\n",
       "37903      unrecorded    4  False     0    unrecord"
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC.to_csv('DOC.csv')\n",
    "LIB.to_csv('LIB.csv')\n",
    "VOCAB.to_csv('VOCAB.csv')\n",
    "TOKEN.to_csv('TOKEN.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: Explore NER tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = TOKEN.groupby(OHCO[:4]).apply(lambda x: x.token_str.str.cat(sep=' '))\\\n",
    "    .to_frame().rename(columns={0:'sent_str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>sent_str</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th>chap_num</th>\n",
       "      <th>para_num</th>\n",
       "      <th>sent_num</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">105</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>Sir Walter Elliot, of Kellynch Hall, in Somers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This was the page at which the favourite volum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>0</th>\n",
       "      <td>\"ELLIOT OF KELLYNCH HALL.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>0</th>\n",
       "      <td>\"Walter Elliot, born March 1, 1760, married, J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>of South Park, in the county of Gloucester, by...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                             sent_str\n",
       "book_id chap_num para_num sent_num                                                   \n",
       "105     1        1        0         Sir Walter Elliot, of Kellynch Hall, in Somers...\n",
       "                          1         This was the page at which the favourite volum...\n",
       "                 2        0                                 \"ELLIOT OF KELLYNCH HALL.\n",
       "                 3        0         \"Walter Elliot, born March 1, 1760, married, J...\n",
       "                          1         of South Park, in the county of Gloucester, by..."
      ]
     },
     "execution_count": 618,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Then arranging his person in the basket, he gave the word for them to hoist him to his perch, Starbuck being the one who secured the rope at last; and afterwards stood near it.\n",
      "\n",
      "[('Then', 'RB'), ('arranging', 'VBG'), ('his', 'PRP$'), ('person', 'NN'), ('in', 'IN'), ('the', 'DT'), ('basket,', 'NN'), ('he', 'PRP'), ('gave', 'VBD'), ('the', 'DT'), ('word', 'NN'), ('for', 'IN'), ('them', 'PRP'), ('to', 'TO'), ('hoist', 'VB'), ('him', 'PRP'), ('to', 'TO'), ('his', 'PRP$'), ('perch,', 'NN'), ('Starbuck', 'NNP'), ('being', 'VBG'), ('the', 'DT'), ('one', 'NN'), ('who', 'WP'), ('secured', 'VBD'), ('the', 'DT'), ('rope', 'NN'), ('at', 'IN'), ('last;', 'NN'), ('and', 'CC'), ('afterwards', 'NNS'), ('stood', 'VBD'), ('near', 'IN'), ('it.', 'NN')]\n",
      "\n",
      "(S\n",
      "  Then/RB\n",
      "  arranging/VBG\n",
      "  his/PRP$\n",
      "  person/NN\n",
      "  in/IN\n",
      "  the/DT\n",
      "  basket,/NN\n",
      "  he/PRP\n",
      "  gave/VBD\n",
      "  the/DT\n",
      "  word/NN\n",
      "  for/IN\n",
      "  them/PRP\n",
      "  to/TO\n",
      "  hoist/VB\n",
      "  him/PRP\n",
      "  to/TO\n",
      "  his/PRP$\n",
      "  perch,/NN\n",
      "  Starbuck/NNP\n",
      "  being/VBG\n",
      "  the/DT\n",
      "  one/NN\n",
      "  who/WP\n",
      "  secured/VBD\n",
      "  the/DT\n",
      "  rope/NN\n",
      "  at/IN\n",
      "  last;/NN\n",
      "  and/CC\n",
      "  afterwards/NNS\n",
      "  stood/VBD\n",
      "  near/IN\n",
      "  it./NN)\n",
      "--------------------------------------------------------------------------------\n",
      "Advancing Deeper Into The Vale, They Encounter Donjalolo\n",
      "\n",
      "[('Advancing', 'VBG'), ('Deeper', 'NNP'), ('Into', 'NNP'), ('The', 'DT'), ('Vale,', 'NNP'), ('They', 'PRP'), ('Encounter', 'NNP'), ('Donjalolo', 'NNP')]\n",
      "\n",
      "(S\n",
      "  Advancing/VBG\n",
      "  (PERSON Deeper/NNP Into/NNP The/DT)\n",
      "  Vale,/NNP\n",
      "  They/PRP\n",
      "  (PERSON Encounter/NNP Donjalolo/NNP))\n",
      "--------------------------------------------------------------------------------\n",
      "In some of his works his style is only surpassed by the unimprovable sentences of Hobbes of Malmsbury, the paragon of perspicuity.\n",
      "\n",
      "[('In', 'IN'), ('some', 'DT'), ('of', 'IN'), ('his', 'PRP$'), ('works', 'VBZ'), ('his', 'PRP$'), ('style', 'NN'), ('is', 'VBZ'), ('only', 'RB'), ('surpassed', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('unimprovable', 'JJ'), ('sentences', 'NNS'), ('of', 'IN'), ('Hobbes', 'NNP'), ('of', 'IN'), ('Malmsbury,', 'NNP'), ('the', 'DT'), ('paragon', 'NN'), ('of', 'IN'), ('perspicuity.', 'NN')]\n",
      "\n",
      "(S\n",
      "  In/IN\n",
      "  some/DT\n",
      "  of/IN\n",
      "  his/PRP$\n",
      "  works/VBZ\n",
      "  his/PRP$\n",
      "  style/NN\n",
      "  is/VBZ\n",
      "  only/RB\n",
      "  surpassed/VBN\n",
      "  by/IN\n",
      "  the/DT\n",
      "  unimprovable/JJ\n",
      "  sentences/NNS\n",
      "  of/IN\n",
      "  (ORGANIZATION Hobbes/NNP)\n",
      "  of/IN\n",
      "  Malmsbury,/NNP\n",
      "  the/DT\n",
      "  paragon/NN\n",
      "  of/IN\n",
      "  perspicuity./NN)\n",
      "--------------------------------------------------------------------------------\n",
      "When in the rear room, set apart for that object, I stand to receive my guests (who, by the way call more, I suspect, to see my chimney than me) I then stand, not so much before, as, strictly speaking, behind my chimney, which is, indeed, the true host.\n",
      "\n",
      "[('When', 'WRB'), ('in', 'IN'), ('the', 'DT'), ('rear', 'JJ'), ('room,', 'NN'), ('set', 'VBN'), ('apart', 'RB'), ('for', 'IN'), ('that', 'DT'), ('object,', 'NN'), ('I', 'PRP'), ('stand', 'VBP'), ('to', 'TO'), ('receive', 'VB'), ('my', 'PRP$'), ('guests', 'NNS'), ('(who,', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('way', 'NN'), ('call', 'NN'), ('more,', 'NN'), ('I', 'PRP'), ('suspect,', 'VBP'), ('to', 'TO'), ('see', 'VB'), ('my', 'PRP$'), ('chimney', 'NN'), ('than', 'IN'), ('me)', 'NN'), ('I', 'PRP'), ('then', 'RB'), ('stand,', 'VBD'), ('not', 'RB'), ('so', 'RB'), ('much', 'JJ'), ('before,', 'NN'), ('as,', 'NN'), ('strictly', 'RB'), ('speaking,', 'VBD'), ('behind', 'IN'), ('my', 'PRP$'), ('chimney,', 'NN'), ('which', 'WDT'), ('is,', 'VBZ'), ('indeed,', 'VBP'), ('the', 'DT'), ('true', 'JJ'), ('host.', 'NN')]\n",
      "\n",
      "(S\n",
      "  When/WRB\n",
      "  in/IN\n",
      "  the/DT\n",
      "  rear/JJ\n",
      "  room,/NN\n",
      "  set/VBN\n",
      "  apart/RB\n",
      "  for/IN\n",
      "  that/DT\n",
      "  object,/NN\n",
      "  I/PRP\n",
      "  stand/VBP\n",
      "  to/TO\n",
      "  receive/VB\n",
      "  my/PRP$\n",
      "  guests/NNS\n",
      "  (who,/VBN\n",
      "  by/IN\n",
      "  the/DT\n",
      "  way/NN\n",
      "  call/NN\n",
      "  more,/NN\n",
      "  I/PRP\n",
      "  suspect,/VBP\n",
      "  to/TO\n",
      "  see/VB\n",
      "  my/PRP$\n",
      "  chimney/NN\n",
      "  than/IN\n",
      "  me)/NN\n",
      "  I/PRP\n",
      "  then/RB\n",
      "  stand,/VBD\n",
      "  not/RB\n",
      "  so/RB\n",
      "  much/JJ\n",
      "  before,/NN\n",
      "  as,/NN\n",
      "  strictly/RB\n",
      "  speaking,/VBD\n",
      "  behind/IN\n",
      "  my/PRP$\n",
      "  chimney,/NN\n",
      "  which/WDT\n",
      "  is,/VBZ\n",
      "  indeed,/VBP\n",
      "  the/DT\n",
      "  true/JJ\n",
      "  host./NN)\n",
      "--------------------------------------------------------------------------------\n",
      "\"Well, what does thou think then of seeing the world?\n",
      "\n",
      "[('\"Well,', 'VB'), ('what', 'WP'), ('does', 'VBZ'), ('thou', 'VB'), ('think', 'VB'), ('then', 'RB'), ('of', 'IN'), ('seeing', 'VBG'), ('the', 'DT'), ('world?', 'NN')]\n",
      "\n",
      "(S\n",
      "  \"Well,/VB\n",
      "  what/WP\n",
      "  does/VBZ\n",
      "  thou/VB\n",
      "  think/VB\n",
      "  then/RB\n",
      "  of/IN\n",
      "  seeing/VBG\n",
      "  the/DT\n",
      "  world?/NN)\n",
      "--------------------------------------------------------------------------------\n",
      "Captain Benwick listened attentively, and seemed grateful for the interest implied; and though with a shake of the head, and sighs which declared his little faith in the efficacy of any books on grief like his, noted down the names of those she recommended, and promised to procure and read them.\n",
      "\n",
      "[('Captain', 'NNP'), ('Benwick', 'NNP'), ('listened', 'VBD'), ('attentively,', 'NNS'), ('and', 'CC'), ('seemed', 'VBD'), ('grateful', 'JJ'), ('for', 'IN'), ('the', 'DT'), ('interest', 'NN'), ('implied;', 'NN'), ('and', 'CC'), ('though', 'IN'), ('with', 'IN'), ('a', 'DT'), ('shake', 'NN'), ('of', 'IN'), ('the', 'DT'), ('head,', 'NN'), ('and', 'CC'), ('sighs', 'NNS'), ('which', 'WDT'), ('declared', 'VBD'), ('his', 'PRP$'), ('little', 'JJ'), ('faith', 'NN'), ('in', 'IN'), ('the', 'DT'), ('efficacy', 'NN'), ('of', 'IN'), ('any', 'DT'), ('books', 'NNS'), ('on', 'IN'), ('grief', 'NN'), ('like', 'IN'), ('his,', 'NN'), ('noted', 'VBD'), ('down', 'IN'), ('the', 'DT'), ('names', 'NNS'), ('of', 'IN'), ('those', 'DT'), ('she', 'PRP'), ('recommended,', 'VBD'), ('and', 'CC'), ('promised', 'VBD'), ('to', 'TO'), ('procure', 'VB'), ('and', 'CC'), ('read', 'VB'), ('them.', 'NN')]\n",
      "\n",
      "(S\n",
      "  (GPE Captain/NNP)\n",
      "  (PERSON Benwick/NNP)\n",
      "  listened/VBD\n",
      "  attentively,/NNS\n",
      "  and/CC\n",
      "  seemed/VBD\n",
      "  grateful/JJ\n",
      "  for/IN\n",
      "  the/DT\n",
      "  interest/NN\n",
      "  implied;/NN\n",
      "  and/CC\n",
      "  though/IN\n",
      "  with/IN\n",
      "  a/DT\n",
      "  shake/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  head,/NN\n",
      "  and/CC\n",
      "  sighs/NNS\n",
      "  which/WDT\n",
      "  declared/VBD\n",
      "  his/PRP$\n",
      "  little/JJ\n",
      "  faith/NN\n",
      "  in/IN\n",
      "  the/DT\n",
      "  efficacy/NN\n",
      "  of/IN\n",
      "  any/DT\n",
      "  books/NNS\n",
      "  on/IN\n",
      "  grief/NN\n",
      "  like/IN\n",
      "  his,/NN\n",
      "  noted/VBD\n",
      "  down/IN\n",
      "  the/DT\n",
      "  names/NNS\n",
      "  of/IN\n",
      "  those/DT\n",
      "  she/PRP\n",
      "  recommended,/VBD\n",
      "  and/CC\n",
      "  promised/VBD\n",
      "  to/TO\n",
      "  procure/VB\n",
      "  and/CC\n",
      "  read/VB\n",
      "  them./NN)\n",
      "--------------------------------------------------------------------------------\n",
      "Elton?\"\n",
      "\n",
      "[('Elton?\"', 'NN')]\n",
      "\n",
      "(S Elton?\"/NN)\n",
      "--------------------------------------------------------------------------------\n",
      "That Lucy was disposed to be jealous of her appeared very probable: it was plain that Edward had always spoken highly in her praise, not merely from Lucy's assertion, but from her venturing to trust her on so short a personal acquaintance, with a secret so confessedly and evidently important.\n",
      "\n",
      "[('That', 'DT'), ('Lucy', 'NNP'), ('was', 'VBD'), ('disposed', 'VBN'), ('to', 'TO'), ('be', 'VB'), ('jealous', 'JJ'), ('of', 'IN'), ('her', 'PRP$'), ('appeared', 'JJ'), ('very', 'RB'), ('probable:', 'IN'), ('it', 'PRP'), ('was', 'VBD'), ('plain', 'NN'), ('that', 'IN'), ('Edward', 'NNP'), ('had', 'VBD'), ('always', 'RB'), ('spoken', 'VBN'), ('highly', 'RB'), ('in', 'IN'), ('her', 'PRP$'), ('praise,', 'NN'), ('not', 'RB'), ('merely', 'RB'), ('from', 'IN'), (\"Lucy's\", 'NNP'), ('assertion,', 'NN'), ('but', 'CC'), ('from', 'IN'), ('her', 'PRP$'), ('venturing', 'NN'), ('to', 'TO'), ('trust', 'VB'), ('her', 'PRP$'), ('on', 'IN'), ('so', 'IN'), ('short', 'JJ'), ('a', 'DT'), ('personal', 'JJ'), ('acquaintance,', 'NN'), ('with', 'IN'), ('a', 'DT'), ('secret', 'JJ'), ('so', 'RB'), ('confessedly', 'RB'), ('and', 'CC'), ('evidently', 'RB'), ('important.', 'VB')]\n",
      "\n",
      "(S\n",
      "  That/DT\n",
      "  (PERSON Lucy/NNP)\n",
      "  was/VBD\n",
      "  disposed/VBN\n",
      "  to/TO\n",
      "  be/VB\n",
      "  jealous/JJ\n",
      "  of/IN\n",
      "  her/PRP$\n",
      "  appeared/JJ\n",
      "  very/RB\n",
      "  probable:/IN\n",
      "  it/PRP\n",
      "  was/VBD\n",
      "  plain/NN\n",
      "  that/IN\n",
      "  (PERSON Edward/NNP)\n",
      "  had/VBD\n",
      "  always/RB\n",
      "  spoken/VBN\n",
      "  highly/RB\n",
      "  in/IN\n",
      "  her/PRP$\n",
      "  praise,/NN\n",
      "  not/RB\n",
      "  merely/RB\n",
      "  from/IN\n",
      "  Lucy's/NNP\n",
      "  assertion,/NN\n",
      "  but/CC\n",
      "  from/IN\n",
      "  her/PRP$\n",
      "  venturing/NN\n",
      "  to/TO\n",
      "  trust/VB\n",
      "  her/PRP$\n",
      "  on/IN\n",
      "  so/IN\n",
      "  short/JJ\n",
      "  a/DT\n",
      "  personal/JJ\n",
      "  acquaintance,/NN\n",
      "  with/IN\n",
      "  a/DT\n",
      "  secret/JJ\n",
      "  so/RB\n",
      "  confessedly/RB\n",
      "  and/CC\n",
      "  evidently/RB\n",
      "  important./VB)\n",
      "--------------------------------------------------------------------------------\n",
      "Catherine, in some amazement, complied, and after remaining a few moments silent, was on the point of reverting to what interested her at that time rather more than anything else in the world, Laurentina's skeleton, when her friend prevented her, by saying, \"For heaven's sake!\n",
      "\n",
      "[('Catherine,', 'NNP'), ('in', 'IN'), ('some', 'DT'), ('amazement,', 'NN'), ('complied,', 'NN'), ('and', 'CC'), ('after', 'IN'), ('remaining', 'VBG'), ('a', 'DT'), ('few', 'JJ'), ('moments', 'NNS'), ('silent,', 'NN'), ('was', 'VBD'), ('on', 'IN'), ('the', 'DT'), ('point', 'NN'), ('of', 'IN'), ('reverting', 'VBG'), ('to', 'TO'), ('what', 'WP'), ('interested', 'JJ'), ('her', 'PRP$'), ('at', 'IN'), ('that', 'DT'), ('time', 'NN'), ('rather', 'RB'), ('more', 'RBR'), ('than', 'IN'), ('anything', 'NN'), ('else', 'RB'), ('in', 'IN'), ('the', 'DT'), ('world,', 'NN'), (\"Laurentina's\", 'NNP'), ('skeleton,', 'NN'), ('when', 'WRB'), ('her', 'PRP$'), ('friend', 'NN'), ('prevented', 'VBD'), ('her,', 'NN'), ('by', 'IN'), ('saying,', 'NN'), ('\"For', 'NNP'), (\"heaven's\", 'NN'), ('sake!', 'NN')]\n",
      "\n",
      "(S\n",
      "  Catherine,/NNP\n",
      "  in/IN\n",
      "  some/DT\n",
      "  amazement,/NN\n",
      "  complied,/NN\n",
      "  and/CC\n",
      "  after/IN\n",
      "  remaining/VBG\n",
      "  a/DT\n",
      "  few/JJ\n",
      "  moments/NNS\n",
      "  silent,/NN\n",
      "  was/VBD\n",
      "  on/IN\n",
      "  the/DT\n",
      "  point/NN\n",
      "  of/IN\n",
      "  reverting/VBG\n",
      "  to/TO\n",
      "  what/WP\n",
      "  interested/JJ\n",
      "  her/PRP$\n",
      "  at/IN\n",
      "  that/DT\n",
      "  time/NN\n",
      "  rather/RB\n",
      "  more/RBR\n",
      "  than/IN\n",
      "  anything/NN\n",
      "  else/RB\n",
      "  in/IN\n",
      "  the/DT\n",
      "  world,/NN\n",
      "  Laurentina's/NNP\n",
      "  skeleton,/NN\n",
      "  when/WRB\n",
      "  her/PRP$\n",
      "  friend/NN\n",
      "  prevented/VBD\n",
      "  her,/NN\n",
      "  by/IN\n",
      "  saying,/NN\n",
      "  \"For/NNP\n",
      "  heaven's/NN\n",
      "  sake!/NN)\n",
      "--------------------------------------------------------------------------------\n",
      "Do not forget me when you are married, that's all.\n",
      "\n",
      "[('Do', 'VBP'), ('not', 'RB'), ('forget', 'VB'), ('me', 'PRP'), ('when', 'WRB'), ('you', 'PRP'), ('are', 'VBP'), ('married,', 'JJ'), (\"that's\", 'NN'), ('all.', 'NN')]\n",
      "\n",
      "(S\n",
      "  Do/VBP\n",
      "  not/RB\n",
      "  forget/VB\n",
      "  me/PRP\n",
      "  when/WRB\n",
      "  you/PRP\n",
      "  are/VBP\n",
      "  married,/JJ\n",
      "  that's/NN\n",
      "  all./NN)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for sent in sents.sample(10).sent_str.values:\n",
    "\n",
    "    print(sent)\n",
    "    print()\n",
    "    \n",
    "    x = nltk.pos_tag(nltk.WhitespaceTokenizer().tokenize(sent))\n",
    "    print(x)\n",
    "    print()\n",
    "    \n",
    "    y = nltk.ne_chunk(x)\n",
    "    print(y)\n",
    "    print('-' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NfMtOiCYDylX",
    "toc-hr-collapsed": false
   },
   "source": [
    "# POS Tagset\n",
    "\n",
    "This a token-level feature -- not a vocab feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FFoFrPTSN4s_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DS5559_Annotations.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
